# ============================================================
# shap_explainer.py
#
# SHAPExplainer (all-in-one) — UPDATED
#
# Key updates requested:
# - Correct samples are written/read from paths where the folder name IS the station:
#     correct_samples_output_path/<STATION>/foreshock|aftershock
# - No "tag" / no extra nesting level (station folder is the experiment container)
# - You can merge correctly-predicted sets across 2+ models (intersection/union)
# - After generating correct samples, it prints class counts
# - You can cap correct samples to N per class (in-place) and it reprints counts
# - Compute SHAP values using ONLY those correct samples (input_path/output_path)
#
# Notes:
# - Do NOT name this file shap.py (would shadow shap package).
# - Works with your CNN2D LightningModule from training.py
# - SHAP backend:
#   * modern SHAP: shap.maskers + shap.Explainer (if available)
#   * legacy fallback: shap.GradientExplainer (if maskers missing)
# ============================================================

import json
import gc
import hashlib
import random
from pathlib import Path

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import matplotlib.pyplot as plt

import shap
from tqdm import tqdm

from training import CNN2D  # your LightningModule


# ----------------------------
# Helpers
# ----------------------------
def get_project_root(marker="preprocessed_dset"):
    here = Path.cwd().resolve()
    for p in [here] + list(here.parents):
        if (p / marker).exists():
            return p
    raise RuntimeError("Project root not found")


def ensure_2d(arr):
    a = np.asarray(arr)

    # squeeze leading singleton dims
    while a.ndim >= 3 and a.shape[0] == 1:
        a = np.squeeze(a, axis=0)

    # common SHAP formats cleanup
    if a.ndim == 4 and a.shape[-1] in (1, 2) and a.shape[-2] in (3, 4):
        a = a[..., 0]

    if a.ndim == 3 and a.shape[-1] in (3, 4):
        a = a.mean(axis=-1)

    if a.ndim != 2:
        raise ValueError(f"Cannot ensure 2D: got {arr.shape} -> {a.shape}")
    return a


def preprocess_for_shap(images: torch.Tensor):
    """
    images: (N,C,H,W) float
    returns uint8 NHWC + original min/max for inversion
    """
    min_val = torch.min(images)
    max_val = torch.max(images)
    x = (images - min_val) / (max_val - min_val + 1e-12)
    x = (x * 255).to(torch.uint8)
    x = x.permute(0, 2, 3, 1)  # (N,H,W,C)
    return x, float(min_val.item()), float(max_val.item())


def inverse_preprocess_for_shap(shap_images: torch.Tensor, original_min: float, original_max: float):
    """
    shap_images: (N,H,W,C) uint8
    returns (N,C,H,W) float
    """
    x = shap_images.permute(0, 3, 1, 2).float()  # (N,C,H,W)
    x = x / 255.0
    x = x * (original_max - original_min) + original_min
    return x


class PngDataset(Dataset):
    def __init__(self, paths, transform):
        self.paths = [Path(p) for p in paths]
        self.transform = transform

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        p = self.paths[idx]
        with Image.open(p) as im:
            im = im.convert("RGB")
            x = self.transform(im)
        return x, p.stem


# ============================================================
# SHAPExplainer
# ============================================================
class SHAPExplainer:
    """
    SHAPExplainer (all-in-one) — FINAL VERSION (as requested)

    Core rules (as per your latest requirements):
    - Correct samples are generated by testing BOTH (or all) models on the station test set.
    - A sample is kept ONLY IF ALL models predict it correctly (intersection).
    - Output structure for correct samples:
        <correct_output_root>/<STATION>/foreshock/*.png
        <correct_output_root>/<STATION>/aftershock/*.png
    - Cap works on that same station folder (no tags).
    - SHAP is computed ONLY on those correct samples, with explicit input/output roots:
        input_path  = correct_output_root
        output_path = shap_output_root
        and saved into:
        <shap_output_root>/<STATION>/foreshock/foreshock/*.npy
        <shap_output_root>/<STATION>/aftershock/aftershock/*.npy
    """

    def __init__(
        self,
        trained_models_dir="trained_models/models",
        preprocessed_dir="preprocessed_dset",
        verbose=True,
    ):
        self.project_root = get_project_root(marker=preprocessed_dir)

        self.models_root = self.project_root / trained_models_dir
        self.pre_root = self.project_root / preprocessed_dir

        self.verbose = bool(verbose)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        self.has_modern_shap = hasattr(shap, "maskers") and hasattr(shap, "Explainer")
        if self.verbose:
            print("[SHAPExplainer]")
            print("  project_root   :", self.project_root)
            print("  pre_root       :", self.pre_root)
            print("  models_root    :", self.models_root)
            print("  device         :", self.device)
            print("  modern_shap?   :", self.has_modern_shap)

    # ----------------------------
    # Discovery / station helpers
    # ----------------------------
    def list_stations(self):
        if not self.pre_root.exists():
            return []
        stations = []
        for p in sorted(self.pre_root.iterdir()):
            if p.is_dir() and (p / "input_dim.npy").exists() and (p / "test").exists():
                stations.append(p.name)
        return stations

    def _station_pre_dir(self, station: str) -> Path:
        return self.pre_root / station

    def _station_model_dir(self, station: str) -> Path:
        return self.models_root / station

    def _load_input_dim(self, pre_dir: Path):
        h, w = np.load(pre_dir / "input_dim.npy").astype(int).tolist()
        return (int(h), int(w))

    def _load_stats(self, model_dir: Path):
        d = json.loads((model_dir / "stats.json").read_text())
        return d["mean"], d["std"]

    def _model_dir_from_path(self, ckpt_or_dir) -> Path:
        """
        Accepts:
          - .../<MODEL_DIR>/best.ckpt
          - .../<MODEL_DIR>/
        Returns:
          MODEL_DIR Path
        """
        p = Path(ckpt_or_dir)
        if p.is_dir():
            if (p / "best.ckpt").exists():
                return p
            raise FileNotFoundError(f"Model dir provided but best.ckpt missing: {p}")
        return p.parent

    def _load_model_from_dir(self, model_dir: Path, input_dim):
        ckpt = model_dir / "best.ckpt"
        if not ckpt.exists():
            raise FileNotFoundError(f"Missing checkpoint: {ckpt}")
        if not (model_dir / "stats.json").exists():
            raise FileNotFoundError(f"Missing stats.json: {model_dir / 'stats.json'}")

        mean, std = self._load_stats(model_dir)

        model = CNN2D.load_from_checkpoint(
            str(ckpt),
            input_dim=input_dim,
            n_classes=2,
            lr=1e-3,
            dropout=0.1,
        )
        model.eval().to(self.device)
        return model, mean, std

    def _load_station_model(self, station: str):
        pre_dir = self._station_pre_dir(station)
        model_dir = self._station_model_dir(station)

        ckpt = model_dir / "best.ckpt"
        if not ckpt.exists():
            raise FileNotFoundError(f"Missing checkpoint: {ckpt}")
        if not (model_dir / "stats.json").exists():
            raise FileNotFoundError(f"Missing stats.json: {model_dir / 'stats.json'}")
        if not (pre_dir / "input_dim.npy").exists():
            raise FileNotFoundError(f"Missing input_dim.npy: {pre_dir / 'input_dim.npy'}")

        input_dim = self._load_input_dim(pre_dir)
        mean, std = self._load_stats(model_dir)

        model = CNN2D.load_from_checkpoint(
            str(ckpt),
            input_dim=input_dim,
            n_classes=2,
            lr=1e-3,
            dropout=0.1,
        )
        model.eval().to(self.device)
        return model, input_dim, mean, std, pre_dir, model_dir

    def _load_ft(self, station: str):
        pre_dir = self._station_pre_dir(station)
        p = pre_dir / "f_t_range.npy"
        if not p.exists():
            raise FileNotFoundError(f"Missing f_t_range.npy: {p}")
        tf = np.load(p).astype(float)
        return [tf[:2], tf[2:]]

    # ----------------------------
    # Count / print helpers
    # ----------------------------
    def _counts_in_station_folder(self, station_folder: Path):
        fo = len(list((station_folder / "foreshock").glob("*.png"))) if (station_folder / "foreshock").exists() else 0
        af = len(list((station_folder / "aftershock").glob("*.png"))) if (station_folder / "aftershock").exists() else 0
        return {"foreshock": fo, "aftershock": af}

    def _print_counts(self, title: str, station_folder: Path):
        c = self._counts_in_station_folder(station_folder)
        print(f"{title}: [foreshock={c['foreshock']}, aftershock={c['aftershock']}]")

    # ============================================================
    # 1) Generate correctly predicted PNGs for a station
    #    Keep ONLY if ALL models predict correctly (intersection)
    # ============================================================
    def generate_correct_samples_multi(
        self,
        station: str,
        model_paths,          # list of ckpt paths OR model dirs (e.g. [station_model, global_model])
        output_path,          # ROOT output, e.g. "outputs/correct_samples"
        overwrite=False,
        max_scan_per_class=None,
    ):
        """
        Reads test PNGs from:
          preprocessed_dir/<station>/test/foreshock|aftershock/*.png

        Writes correct PNGs to:
          <output_path>/<station>/foreshock/*.png
          <output_path>/<station>/aftershock/*.png
          <output_path>/<station>/manifest.json

        Keeps a file ONLY if ALL models predict it correctly.
        """
        if isinstance(model_paths, (str, Path)):
            model_paths = [model_paths]
        model_paths = [Path(p) for p in model_paths]
        if len(model_paths) < 2:
            raise ValueError("Pass at least 2 model paths (e.g. station + global).")

        pre_dir = self._station_pre_dir(station)
        if not pre_dir.exists():
            raise FileNotFoundError(f"Missing station pre_dir: {pre_dir}")

        input_dim = self._load_input_dim(pre_dir)

        # Load each model with its own normalization (stats.json)
        models = []
        for p in model_paths:
            mdir = self._model_dir_from_path(p)
            model, mean, std = self._load_model_from_dir(mdir, input_dim=input_dim)
            tfm = transforms.Compose([
                transforms.Resize(input_dim),
                transforms.ToTensor(),
                transforms.Normalize(mean=mean, std=std),
            ])
            models.append({"model": model, "tfm": tfm, "model_dir": mdir})

        # Output station folder inside output_path
        out_root = Path(output_path)
        out_station = out_root / station
        fo_out = out_station / "foreshock"
        af_out = out_station / "aftershock"
        fo_out.mkdir(parents=True, exist_ok=True)
        af_out.mkdir(parents=True, exist_ok=True)

        if overwrite:
            for d in (fo_out, af_out):
                for fp in d.glob("*.png"):
                    fp.unlink(missing_ok=True)

        # Input test folders
        test_root = pre_dir / "test"
        fo_in = test_root / "foreshock"
        af_in = test_root / "aftershock"

        saved = {"foreshock": 0, "aftershock": 0}

        def all_models_correct(png_path: Path, true_label: int) -> bool:
            """
            Open image once; for each model use its own transform.
            Keep ONLY if all models predict true_label.
            """
            with Image.open(png_path) as im:
                im = im.convert("RGB")
                for item in models:
                    x = item["tfm"](im).unsqueeze(0).to(self.device)
                    with torch.no_grad():
                        logits = item["model"](x)
                        pred = int(torch.argmax(logits, dim=1).item())
                    if pred != true_label:
                        return False
            return True

        def scan_class(in_dir: Path, out_dir: Path, true_label: int, key: str):
            nonlocal saved
            if not in_dir.exists():
                return
            files = sorted(in_dir.glob("*.png"))
            if max_scan_per_class is not None:
                files = files[: int(max_scan_per_class)]

            for p in files:
                if all_models_correct(p, true_label):
                    (out_dir / p.name).write_bytes(p.read_bytes())
                    saved[key] += 1

        scan_class(fo_in, fo_out, 0, "foreshock")
        scan_class(af_in, af_out, 1, "aftershock")

        # Manifest
        (out_station / "manifest.json").write_text(json.dumps({
            "station": station,
            "mode": "intersection_all_models_correct",
            "models": [str(item["model_dir"].resolve()) for item in models],
            "saved": saved,
            "output_path_root": str(out_root.resolve()),
        }, indent=2))

        # Print counts
        self._print_counts(f"[OK] {station} common-correct samples generated", out_station)

        # Cleanup models
        for item in models:
            del item["model"]
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return out_station

    # ============================================================
    # 2) Cap correctly predicted samples to N per class (in-place)
    # ============================================================
    def cap_correct_samples(
        self,
        station: str,
        input_path,          # ROOT correct samples, e.g. "outputs/correct_samples"
        max_per_class: int,
        seed=42,
    ):
        """
        Caps in-place:
          <input_path>/<station>/foreshock/*.png
          <input_path>/<station>/aftershock/*.png

        Prints lengths after cap.
        """
        random.seed(seed)

        station_folder = Path(input_path) / station
        if not station_folder.exists():
            raise FileNotFoundError(station_folder)

        for cls in ["foreshock", "aftershock"]:
            d = station_folder / cls
            if not d.exists():
                continue
            files = sorted(d.glob("*.png"))
            if len(files) > max_per_class:
                random.shuffle(files)
                for p in files[max_per_class:]:
                    p.unlink(missing_ok=True)

        # Update manifest if exists
        man = station_folder / "manifest.json"
        if man.exists():
            m = json.loads(man.read_text())
            m["capped_to"] = int(max_per_class)
            m["counts_after_cap"] = self._counts_in_station_folder(station_folder)
            man.write_text(json.dumps(m, indent=2))

        self._print_counts(f"[OK] {station} capped correctly-predicted", station_folder)
        return station_folder

    # ============================================================
    # 3) Compute SHAP on correct samples (input/output roots)
    # ============================================================
    def compute_shap_on_correct_samples(
        self,
        station: str,
        shap_model_path,     # ckpt path OR model dir (the model you want to explain)
        input_path,          # ROOT correct samples, e.g. "outputs/correct_samples"
        output_path,         # ROOT shap output, e.g. "outputs/shap_values_global"
        max_evals=500,
        masker_settings="inpaint_telea",
        len_label_channels=1,
        overwrite=False,
        batch_size_explainer=50,
    ):
        """
        Reads correct PNGs from:
          <input_path>/<station>/foreshock/*.png
          <input_path>/<station>/aftershock/*.png

        Writes SHAP tensors to:
          <output_path>/<station>/foreshock/foreshock/*.npy
          <output_path>/<station>/aftershock/aftershock/*.npy
        """
        pre_dir = self._station_pre_dir(station)
        if not pre_dir.exists():
            raise FileNotFoundError(f"Missing station pre_dir: {pre_dir}")
        input_dim = self._load_input_dim(pre_dir)

        # Load model-to-explain + its stats
        mdir = self._model_dir_from_path(shap_model_path)
        model, mean, std = self._load_model_from_dir(mdir, input_dim=input_dim)

        in_station = Path(input_path) / station
        out_station = Path(output_path) / station

        if not in_station.exists():
            raise FileNotFoundError(in_station)
        out_station.mkdir(parents=True, exist_ok=True)

        tfm = transforms.Compose([
            transforms.Resize(input_dim),
            transforms.ToTensor(),
            transforms.Normalize(mean=mean, std=std),
        ])

        for cls in ["foreshock", "aftershock"]:
            img_dir = in_station / cls
            if not img_dir.exists():
                if self.verbose:
                    print(f"[SKIP] missing input folder: {img_dir}")
                continue

            imgs = sorted(img_dir.glob("*.png"))
            if len(imgs) == 0:
                if self.verbose:
                    print(f"[SKIP] no pngs in: {img_dir}")
                continue

            out_dir = out_station / cls / cls
            out_dir.mkdir(parents=True, exist_ok=True)

            ds = PngDataset(imgs, tfm)
            dl = DataLoader(ds, batch_size=1, shuffle=False, num_workers=0)

            if self.verbose:
                print(f"[{station}] SHAP | {cls}: n={len(ds)} -> {out_dir}")

            for x, trace_name in tqdm(
                dl,
                desc=f"{station} SHAP {cls}",
                total=len(dl),
                disable=not self.verbose,
            ):
                trace_name = trace_name[0]
                out_file = out_dir / f"{trace_name}.npy"
                if out_file.exists() and not overwrite:
                    continue

                sv = self._compute_shap_one(
                    model=model,
                    x=x,
                    input_dim=input_dim,
                    max_evals=max_evals,
                    batch_size=batch_size_explainer,
                    masker_settings=masker_settings,
                    len_label_channels=len_label_channels,
                )
                np.save(out_file, sv)

        del model
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        return out_station

    # ============================================================
    # SHAP backend (modern / legacy)
    # ============================================================
    def _compute_shap_one(self, model, x, input_dim, max_evals, batch_size, masker_settings, len_label_channels):
        """
        x: (1,C,H,W)
        returns SHAP values (numpy array). Shape depends on backend.
        """
        if self.has_modern_shap:
            images_to_explain, original_min, original_max = preprocess_for_shap(x)

            def model_fn(nhwc_np):
                xx = torch.from_numpy(nhwc_np).to(self.device)
                xx = inverse_preprocess_for_shap(xx, original_min, original_max)
                with torch.no_grad():
                    out = model(xx)
                return out.detach().cpu().numpy()

            masker = shap.maskers.Image(masker_settings, (*input_dim, 3))
            explainer = shap.Explainer(model_fn, masker, output_names=["Foreshock", "Aftershock"])

            sv = explainer(
                images_to_explain.cpu().numpy(),
                max_evals=max_evals,
                batch_size=batch_size,
                outputs=shap.Explanation.argsort.flip[:len_label_channels],
            )
            return np.array(sv.values)

        # Legacy fallback (GradientExplainer)
        x = x.to(self.device)
        x.requires_grad = True
        background = torch.zeros_like(x)
        explainer = shap.GradientExplainer(model, background)
        shap_values = explainer.shap_values(x)  # list per output

        sv = shap_values[0]
        if isinstance(sv, torch.Tensor):
            sv = sv.detach().cpu().numpy()

        # Normalize to something plottable-ish
        sv = sv[0].transpose(1, 2, 0)
        return sv

    # ============================================================
    # Means (computed from SHAP output root + station)
    # ============================================================
    def compute_means_from_shap_output(
        self,
        station: str,
        input_path,          # ROOT shap output, e.g. "outputs/shap_values_global"
        overwrite=True,
    ):
        """
        Reads:
          <input_path>/<station>/foreshock/foreshock/*.npy
          <input_path>/<station>/aftershock/aftershock/*.npy

        Writes:
          <input_path>/<station>/foreshock/avg_p_pre.npy
          <input_path>/<station>/aftershock/avg_p_post.npy
        """
        shap_station = Path(input_path) / station
        fo_folder = shap_station / "foreshock" / "foreshock"
        af_folder = shap_station / "aftershock" / "aftershock"

        def mean_of_folder(folder: Path):
            files = sorted(folder.glob("*.npy"))
            maps = []
            for p in files:
                try:
                    a = np.load(p, allow_pickle=True)
                    m = ensure_2d(a)
                    maps.append(m)
                except Exception:
                    pass
            if len(maps) == 0:
                return None
            shapes = {m.shape for m in maps}
            if len(shapes) != 1:
                raise ValueError(f"Shape mismatch in {folder}: {shapes}")
            return np.mean(np.stack(maps, axis=0), axis=0)

        if fo_folder.exists():
            m = mean_of_folder(fo_folder)
            if m is not None:
                out = shap_station / "foreshock" / "avg_p_pre.npy"
                out.parent.mkdir(parents=True, exist_ok=True)
                if overwrite or not out.exists():
                    np.save(out, m)
                if self.verbose:
                    print(f"[OK] {station} saved {out} shape={m.shape}")

        if af_folder.exists():
            m = mean_of_folder(af_folder)
            if m is not None:
                out = shap_station / "aftershock" / "avg_p_post.npy"
                out.parent.mkdir(parents=True, exist_ok=True)
                if overwrite or not out.exists():
                    np.save(out, m)
                if self.verbose:
                    print(f"[OK] {station} saved {out} shape={m.shape}")

        return shap_station

    # ============================================================
    # Plotting from SHAP output root + station
    # ============================================================
    def _load_mean_map_from_shap_output(self, station: str, shap_output_root, which):
        shap_station = Path(shap_output_root) / station
        if which == "pre":
            p = shap_station / "foreshock" / "avg_p_pre.npy"
        else:
            p = shap_station / "aftershock" / "avg_p_post.npy"
        if not p.exists():
            raise FileNotFoundError(f"Missing mean map: {p}")
        return ensure_2d(np.load(p, allow_pickle=True))

    def _imshow_map(self, ax, m, ft, vmin, vmax, alpha_norm, title, show_ylabel):
        f, t = ft
        ax.set_title(title)
        ax.axvline(5.0, c="black", lw=2, alpha=0.3)
        im = ax.imshow(
            m,
            cmap="coolwarm",
            alpha=np.clip(np.abs(m) / alpha_norm, 0, 1),
            aspect="auto",
            origin="lower",
            extent=[*t, *f],
            vmin=vmin,
            vmax=vmax,
        )
        if show_ylabel:
            ax.set_ylabel("Frequency [Hz]")
        else:
            ax.set_yticklabels([])
        ax.set_xlabel("Time [s]")
        return im

    def plot_station_single_from_shap_output(self, station: str, shap_output_root, which, ft=None, symmetric_cbar=True, figsize=(14, 5)):
        if ft is None:
            ft = self._load_ft(station)
        m = self._load_mean_map_from_shap_output(station, shap_output_root, which)

        vmin, vmax = float(m.min()), float(m.max())
        if symmetric_cbar:
            mm = max(abs(vmin), abs(vmax))
            vmin, vmax = -mm, mm
        alpha_norm = float(np.max(np.abs(m))) if np.max(np.abs(m)) > 0 else 1.0

        fig, ax = plt.subplots(figsize=figsize)
        im = self._imshow_map(ax, m, ft, vmin, vmax, alpha_norm, f"{station} | {which}", True)
        cb = plt.colorbar(im, ax=ax, orientation="horizontal", pad=0.1)
        cb.set_label("Contribution")
        plt.show()

    def plot_station_both_from_shap_output(self, station: str, shap_output_root, ft=None, symmetric_cbar=True, figsize=(16, 6)):
        if ft is None:
            ft = self._load_ft(station)

        pre = self._load_mean_map_from_shap_output(station, shap_output_root, "pre")
        post = self._load_mean_map_from_shap_output(station, shap_output_root, "post")

        vals = np.concatenate([pre.ravel(), post.ravel()])
        vmin, vmax = float(vals.min()), float(vals.max())
        if symmetric_cbar:
            mm = max(abs(vmin), abs(vmax))
            vmin, vmax = -mm, mm
        alpha_norm = float(np.max(np.abs(vals))) if np.max(np.abs(vals)) > 0 else 1.0

        fig, axes = plt.subplots(1, 2, figsize=figsize, sharey=True)
        fig.suptitle(f"{station} | pre vs post")

        _ = self._imshow_map(axes[0], pre, ft, vmin, vmax, alpha_norm, "pre (foreshock)", True)
        im2 = self._imshow_map(axes[1], post, ft, vmin, vmax, alpha_norm, "post (aftershock)", False)

        cb = plt.colorbar(im2, ax=axes, orientation="horizontal", pad=0.1)
        cb.set_label("Contribution")
        plt.show()

    def load_mean_shap_tensor_new(self, root, cls, station, ext="npy"):
        """
        NEW expected path (your updated repo layout):

        <root>/<STATION>/foreshock/avg_p_pre.npy
        <root>/<STATION>/aftershock/avg_p_post.npy

        Where:
        cls == "foreshock"  -> avg_p_pre.npy
        cls == "aftershock" -> avg_p_post.npy
        """
        root = Path(root)
        station = str(station)

        if cls not in ("foreshock", "aftershock"):
            raise ValueError("cls must be 'foreshock' or 'aftershock'")

        fname = f"avg_p_{'pre' if cls == 'foreshock' else 'post'}.{ext}"
        path = root / station / cls / fname
        if not path.exists():
            raise FileNotFoundError(f"Missing file: {path}")
        return np.load(path, allow_pickle=True)


    def ensure_2d_local(self, arr, name=""):
        """
        Minimal ensure_2d compatible with your previous logic.
        (Use your ensure_2d if you already have it imported.)
        """
        a = np.asarray(arr)
        while a.ndim >= 3 and a.shape[0] == 1:
            a = np.squeeze(a, axis=0)
        if a.ndim == 3 and a.shape[-1] in (3, 4):
            a = a.mean(axis=-1)
        if a.ndim != 2:
            raise ValueError(f"Cannot ensure 2D for {name}: got {a.shape}")
        return a


    def plot_grid_mean_shap_new(
        self,
        root,
        stations,
        ft,
        ext="npy",
        figsize=(24, 6),
        cmap="coolwarm",
        symmetric_cbar=True,
        p_arrival_t=5.0,
        plot_title=None,
        normalize_all=False,     # NEW: if True -> one global vmin/vmax + alpha_norm for all stations
        save_path=None,          # if provided -> save figure
        dpi=300,
        title_y=1.08,            # NEW: controls space between title and plots (bigger = more space)
        top=0.85,                # NEW: extra top margin for suptitle
    ):
        """
        Grid plot: 2 rows (foreshock/aftershock) x N columns (stations) + one colorbar per column.

        Reads mean maps from NEW repo layout:
        <root>/<STATION>/foreshock/avg_p_pre.npy
        <root>/<STATION>/aftershock/avg_p_post.npy

        Normalization:
        - normalize_all=False (default): vmin/vmax and alpha_norm computed PER STATION (column),
            using BOTH classes for that station (same as your old code).
        - normalize_all=True: vmin/vmax and alpha_norm computed ONCE using ALL stations and BOTH classes,
            so all columns share the same scale (better for absolute comparisons).
        """
        if ft is None:
            raise ValueError("ft is required: ft = ((fmin,fmax),(tmin,tmax))")
        f, t = ft

        CLASSES = ["foreshock", "aftershock"]
        STATIONS = list(stations)
        if len(STATIONS) == 0:
            raise ValueError("stations is empty")

        # -------------------------
        # 1) LOAD ALL TENSORS FIRST
        # -------------------------
        tensors = {cls: {} for cls in CLASSES}

        # global accumulators
        global_vals = []
        global_abs = []

        # per-station accumulators (used when normalize_all=False)
        col_vals = {st: [] for st in STATIONS}
        col_abs = {st: [] for st in STATIONS}

        for st in STATIONS:
            for cls in CLASSES:
                ten = self.load_mean_shap_tensor_new(root, cls, st, ext=ext)
                ten = self.ensure_2d_local(ten, name=f"{cls}-{st}")
                tensors[cls][st] = ten

                flat = ten.ravel()
                absf = np.abs(flat)

                global_vals.append(flat)
                global_abs.append(absf)

                col_vals[st].append(flat)
                col_abs[st].append(absf)

        global_vals = np.concatenate(global_vals)
        global_abs = np.concatenate(global_abs)

        # -------------------------
        # 2) GLOBAL NORMALIZATION
        # -------------------------
        if symmetric_cbar:
            m = max(abs(float(global_vals.min())), abs(float(global_vals.max())))
            global_vmin, global_vmax = -m, m
        else:
            global_vmin, global_vmax = float(global_vals.min()), float(global_vals.max())

        global_alpha_norm = float(np.max(global_abs)) if np.max(global_abs) > 0 else 1.0

        # -------------------------
        # 3) FIGURE LAYOUT
        # -------------------------
        fig = plt.figure(figsize=figsize, constrained_layout=True)

        # add more room for title
        fig.subplots_adjust(top=top)

        gs = fig.add_gridspec(nrows=3, ncols=len(STATIONS), height_ratios=[1, 1, 0.06])
        axes = [[fig.add_subplot(gs[r, c]) for c in range(len(STATIONS))] for r in range(2)]
        cbar_axes = [fig.add_subplot(gs[2, c]) for c in range(len(STATIONS))]

        # -------------------------
        # 4) PLOT
        # -------------------------
        for c, st in enumerate(STATIONS):
            # choose scaling (global or per-station)
            if normalize_all:
                vmin, vmax = global_vmin, global_vmax
                alpha_norm = global_alpha_norm
            else:
                vals = np.concatenate(col_vals[st])
                absvals = np.concatenate(col_abs[st])

                if symmetric_cbar:
                    mm = max(abs(float(vals.min())), abs(float(vals.max())))
                    vmin, vmax = -mm, mm
                else:
                    vmin, vmax = float(vals.min()), float(vals.max())

                alpha_norm = float(np.max(absvals)) if np.max(absvals) > 0 else 1.0

            last_im = None

            for r, cls in enumerate(CLASSES):
                ax = axes[r][c]
                ten = self.ensure_2d_local(tensors[cls][st], name=f"{cls}-{st}")

                alpha = np.clip(np.abs(ten) / alpha_norm, 0, 1)

                ax.axvline(p_arrival_t, c="black", lw=2, alpha=0.3)
                last_im = ax.imshow(
                    ten,
                    cmap=cmap,
                    alpha=alpha,
                    aspect="auto",
                    origin="lower",
                    extent=[*t, *f],
                    vmin=vmin,
                    vmax=vmax,
                )

                # station title only on top row
                if r == 0:
                    ax.set_title(st, fontsize=12)

                # y-label only on first column
                if c == 0:
                    ax.set_ylabel(f"{cls.capitalize()}\nFrequency [Hz]")
                else:
                    ax.set_yticklabels([])

                # x-label only on bottom row
                if r == 1:
                    ax.set_xlabel("Time [s]")
                else:
                    ax.set_xticklabels([])

            # column-specific colorbar (shared for the two rows)
            cb = fig.colorbar(last_im, cax=cbar_axes[c], orientation="horizontal")
            cb.set_label("Contribution", fontsize=9)
            cb.ax.tick_params(labelsize=8)

        # -------------------------
        # 5) TITLE + SAVE + SHOW
        # -------------------------
        if plot_title is not None:
            fig.suptitle(plot_title, fontsize=16, y=title_y)

        if save_path is not None:
            save_path = Path(save_path)
            save_path.parent.mkdir(parents=True, exist_ok=True)
            fig.savefig(save_path, dpi=dpi, bbox_inches="tight")
            if self.verbose:
                print(f"[OK] figure saved to: {save_path.resolve()}")

        plt.show()
